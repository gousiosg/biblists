@inproceedings{BSSRGA20,
  author    = {Arnar Briem, J. and Smit, J. and Sellik, H. and Rapoport, P. and Gousios, G., and Aniche, M.},
  booktitle = {The 2nd Workshop on Testing for Deep Learning and Deep Learning for Testing (DeepTest)},
  title     = {OffSide: Learning to Identify Mistakes in Boundary Conditions},
  year      = {2020},
  location  = {Seoul, South Korea},
  doi       = {10.1145/3387940.3391464},
  abstract  = {
    Mistakes in boundary conditions are the cause of many bugs in software. 
    These mistakes happen when, e.g., developers make use of '<' or '>' in 
    cases where they should have used '<=' or '>='. Mistakes in boundary 
    conditions are often hard to find and manually detecting them might be 
    very time-consuming for developers. While researchers have been proposing
    techniques to cope with mistakes in the boundaries for a long time, the 
    automated detection of such bugs still remains a challenge. We conjecture
    that, for a tool to be able to precisely identify mistakes in boundary 
    conditions, it should be able to capture the overall context of the 
    source code under analysis. In this work, we propose a deep learning 
    model that learn mistakes in boundary conditions and, later, is able to 
    identify them in unseen code snippets. We train and test a model on over 
    1.5 million code snippets, with and without mistakes in different boundary 
    conditions. Our model shows an accuracy from 55% up to 87%. The model is 
    also able to detect 24 out of 41 real-world bugs; however, with a high 
    false positive rate. The existing state-of-the-practice linter tools are 
    not able to detect any of the bugs. We hope this paper can pave the road 
    towards deep learning models that will be able to support developers in 
    detecting mistakes in boundary conditions.
  },
  keywords  = {machine learning for software testing, software testing, boundary testing, machine learning for software engineering, software engineering, deep learning for software testing},
  isbn      = {9781450379632},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  series    = {ICSEW'20},
  url       = {/pub/offside-mistakes-boundary-conditions.pdf},
}

@inproceedings{ACGZ20,
  author    = {P. {Abate} and R. {Di Cosmo} and G. {Gousios} and S. {Zacchiroli}},
  booktitle = {The 27th IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)},
  title     = {Dependency Solving Is Still Hard, but We Are Getting Better at It},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {547-551},
  location  = {London, Ontario, Canada},
  doi       = {10.1109/SANER48275.2020.9054837},
  url       = {/pub/dependency-solving-hard-getting-better.pdf}
}

@inproceedings{GKKPG18,
  author    = {Gerasimou, Simos and Kechagia, Maria and Kolovos, Dimitris and Paige, Richard and Gousios, Georgios},
  title     = {On Software Modernisation Due to Library Obsolescence},
  booktitle = {Proceedings of the 2Nd International Workshop on API Usage and Evolution},
  series    = {WAPI '18},
  year      = {2018},
  isbn      = {978-1-4503-5754-8},
  location  = {Gothenburg, Sweden},
  pages     = {6--9},
  numpages  = {4},
  url       = {http://doi.acm.org/10.1145/3194793.3194798},
  doi       = {10.1145/3194793.3194798},
  acmid     = {3194798},
  publisher = {ACM},
  address   = {New York, NY, USA},
  keywords  = {application programming interfaces, library evolution, software libraries, software modernisation, visualisation}
}

@inproceedings{GSV16,
  author    = {Gousios, Georgios and Safaric, Dominik and Visser, Joost},
  title     = {Streaming Software Analytics},
  booktitle = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
  series    = {BIGDSE '16},
  year      = {2016},
  isbn      = {978-1-4503-4152-3},
  location  = {Austin, Texas},
  pages     = {8--11},
  numpages  = {4},
  url       = {/pub/streaming-soft-analytics.pdf},
  doi       = {10.1145/2896825.2896832},
  acmid     = {2896832},
  publisher = {ACM},
  address   = {New York, NY, USA},
  keywords  = {data mining, software analytics, streaming}
}

@inproceedigns{BLPGZ16,
  author    = {Moritz Beller and Igor Levaja and Annibale Panichella and Georgios Gousios and Andy Zaidman},
  title     = {How to Catch ’Em All: WatchDog, a Family of IDE Plug-Ins to Assess Testing},
  booktitle = {Proceedings of the 3rd International Workshop on Software Engineering Research and Industrial Practice},
  year      = 2016,
  abstract  = {
    As software engineering researchers, we are also zealous tool smiths.
    Building a research prototype is often a daunting task, let alone building a
    industry-grade family of tools supporting multiple platforms to ensure the
    generalizability of our results. In this paper, we give advice to academic
    and industrial tool smiths on how to design and build an easy-to-maintain
    architecture capable of supporting multiple integrated development
    environments (IDEs). Our experiences stem from WatchDog, a multi-IDE
    infrastructure that assesses developer testing activities in vivo and that
    over 2,000 registered developers use. To these software engineering
    practitioners, WatchDog, provides real-time and aggregated feedback in the
    form of individual testing reports.
  },
  url       = {/pub/how-to-catchem-all.pdf}
}

@inproceedings{CDGHH15,
  author    = {Guanliang Chen and Dan Davis and Georgios Gousios and Claudia Hauff and Geert-Jan Houben},
  title     = {Learning transfer: does it take place?},
  booktitle = {Proceedings of the 2nd Workshop of Learning With MOOCs},
  year      = {2015},
  month     = {Oct},
  location  = {NY, USA},
  abstract  = {
    The rising number of MOOCs enable people to advance their knowledge and
    competencies in a wide range of fields. Learning though is only the first
    step, the transfer of the taught concepts into practice is equally important
    and often ne- glected in the investigation of MOOCs. In this paper, we
    consider the specific case of FP101x (a functional programming MOOC on
    EdX) and the extent to which learners alter their programming behaviour
    after having taken the course. We find the practical uptake six months after
    the course to be small: only ~3\% of learners that were not applying
    functional concepts in their programming practice before the start of
    FP101x, begin using them.
  },
  url       = {/pub/learning-transfer.pdf}
}

@inproceedings{MKPLGS14,
  author    = {Dimitris Mitropoulos and Georgios Gousios and Panagiotis Papadopoulos and Vassilios Karakoidas and Panos Louridas and Diomidis Spinellis},
  title     = {The Vulnerability Dataset of a Large Software Ecosystem},
  booktitle = {Proceedings of the 3rd International Workshop on Building Analysis Datasets and Gathering Experience Returns for Security},
  year      = {2014},
  month     = {September},
  location  = {Wroclaw, Poland},
  publisher = {IEEE Computer Society},
  url       = {/pub/maven-bug-dataset.pdf}
}

@inproceedings{MKLGS13,
  author    = {Dimitris Mitropoulos and Vassilios Karakoidas and Panos Louridas and Georgios Gousios and Diomidis Spinellis},
  title     = {Dismal Code: Studying the Evolution of Security Bugs},
  year      = {2013},
  month     = {Oct},
  booktitle = { {LASER} '13: Proceedings of the 2013 Workshop on Learning from Authoritative Security Experiment Results},
  address   = {Arlington, VA},
  isbn      = {978-1-931971-06-5},
  pages     = {37--48},
  abstract  = {
    Security bugs are critical programming errors that can lead to serious
    vulnerabilities in software. Such bugs may allow an attacker to take over an
    application, steal data or prevent the application from working at all.  We
    used the projects stored in the Maven repository to study the
    characteristics of security bugs individually and in relation to other
    software bugs. Specifically, we studied the evolution of security bugs
    through time. In addition, we examined their persistence and their
    relationship with a) the size of the corresponding version, and b) other bug
    categories.  We analyzed every project version of the Maven repository by
    using FindBugs, a popular static analysis tool.  To see how security bugs
    evolve over time we took advantage of the repository's project history and
    dependency data.  Our results indicate that there is no simple rule
    governing the number of security bugs as a project evolves.  In particular,
    we cannot say that across projects security-related defect counts increase
    or decrease significantly over time.  Furthermore, security bugs are not
    eliminated in a way that is particularly different from the other bugs. In
    addition, the relation of security bugs with a project's size appears to be
    different from the relation of the bugs coming from other categories.
    Finally, even if bugs seem to have similar behaviour, severe security bugs
    seem to be unassociated with other bug categories.  Our findings indicate
    that further research should be done to analyze the evolution of security
    bugs. Given the fact that our experiment included only Java projects,
    similar research could be done for another ecosystem. Finally, the fact that
    projects have their own idiosyncrasies concerning security bugs, could help
    us find the common characteristics of the projects where security bugs
    increase over time.
  },
  url       = {/pub/evolution-security-bugs.pdf}
}

@inproceedings{MGS12,
  title     = {Measuring the Occurrence of Security-Related Bugs through Software
    Evolution},
  author    = {Dimitris Mitropoulos and Georgios Gousios and Diomidis
    Spinellis},
  booktitle = {PCI 2012: Proceedings of 16th Panhellenic Conference on
    Informatics},
  publisher = {IEEE Computer Society},
  year      = {2012},
  doi       = {10.1109/PCi.2012.15},
  pages     = {117--122},
  abstract  = {A security-related bug is a programming error that intro- duces a
    potentially exploitable weakness into a computer system. This weakness could
    lead to a security breach with unfortunate consequences. Version control
    systems provide an accurate historical record of the software code’s evolu-
    tion. In this paper we examine the frequency of the security-related bugs
    throughout the evolution of a software project by applying the FindBugs static
    analyzer on all versions of its revision history. We have applied our approach
    on four projects and we have come out with some interesting results including
    the fact that the number of the security-related bugs increase as the project
    evolves.},
  url       = {/pub/measuring-the-occurrence-of-security-related-bugs-through-software-evolution.pdf}
}

@inproceedings{KGSP09,
  author    = {Eirini Kalliamvakou and Georgios Gousios and Diomidis Spinellis and Nancy Pouloudi},
  booktitle = {MCIS 2009: 4th {M}editerranean Conference on Information Systems},
  editor    = {Poulymenakou, A. and Pouloudi, N. and Pramatari, K.},
  location  = {Athens, Greece},
  month     = sep,
  pages     = {600--611},
  title     = {Measuring Developer Contribution from Software Repository Data},
  year      = 2009,
  abstract  = {
    Our work is concerned with an enriched perspective of what constitutes
    developer contribution in software infrastructures supporting incremental
    development and distributed software projects. We use the term
    “contribution” to express the combination of all the actions a developer has
    performed during the development process and propose a model for calculating
    this individually for developers participating in a software project. Our
    approach departs from the traditional practice of only measuring the
    contribution to the final outcome (the code) and puts emphasis additionally
    on other activities that do not directly affect the product itself but are
    essential to the development process. We use the Open Source Software (OSS)
    context to take advantage of the public availability of data in software
    repositories. In this paper, we present our method of calculation and its
    system implementation and we apply our measurements on various projects from
    the GNOME ecosystem.
    },
  url       = {/pub/measuring-developer-contribution-from-software-repository-data.pdf}
}

@inproceedings{GS08,
  author    = {Georgios Gousios and Diomidis Spinellis},
  booktitle = {Proceedings of the 12th Pan-Hellenic Conference on Informatics},
  title     = {Java Performance Evaluation Using External Instrumentation},
  year      = {2008},
  location  = {Samos, Greece},
  doi       = {10.1109/PCI.2008.14},
  pages     = {173-177},
  month     = {Aug},
  abstract  = {
    The performance of programs written in the Java pro- gramming language is
    not trivial to analyse. The Java Vir- tual Machine hides the details of
    bytecode execution while not providing an accessible profiling mechanism.
    Most tools used for Java performance evaluations are based on sampling and
    only present engineers with sampled data aggregations. In this paper, we
    present the Java DTrace Toolkit, a collection of scripts that is
    specifically designed to assist engineers in identifying the roots of
    various perfor- mance problems observed with other tools.
  },
  url       = {/pub/java-performance-evaluation-using-external-instrumentation.pdf}
}

@inproceedings{GKSLVS07,
  author    = {Georgios Gousios and Vassilios Karakoidas and Konstantinos Stroggylos and Panagiotis Louridas and Vasileios Vlachos and Diomidis Spinellis},
  booktitle = {Proceedings of the 11th Panhellenic Conference on Informatics},
  day       = {18--20},
  location  = {Patras, Greece},
  month     = may,
  title     = {Software Quality Assessment of Open Source Software},
  year      = {2007},
  abstract  = {
    The open source software ecosystem comprises more than a hundred thousand
    applications of varying quality. Individuals and organizations wishing to
    use open source software packages have scarce objective data to evaluate
    their quality. However, open source development projects by definition allow
    anybody to read, and therefore evaluate their source code. In addition, most
    projects also publish process-related artefacts, such as bug databases,
    mailing lists, and configuration management system logs. The software
    quality observatory is a platform that uses these product and process data
    sources to automatically evaluate the quality of open source projects. A
    plugin-based service-oriented architecture allows the mixing and matching of
    metrics extraction suites, source code repositories, and transformation
    filters. The resulting platform is aimed at IT consultants and managers, the
    open source community, and researchers.
  },
  url       = {/pub/software-quality-assessment-of-open-source-software.pdf}
}
