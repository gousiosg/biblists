@inproceedings{G18,
  author = {Georgios Gousios},
  title = {Big Data Software Analytics with Apache Spark},
  booktitle = {Proceedings of the 40th International Conference on Software Engineering Companion},
  series = {ICSE-C '18},
  year = {2018},
  location = {Gothenburg, Sweden},
  abstract = {

    At the beginning of every research effort, researchers in empirical software
      engineering have to go through the processes of extracting data from raw
      data sources and transforming them to what their tools expect as inputs.
      This step is time consuming and error prone, while the produced artifacts
      (code, intermediate datasets) are usually not of scientific value.  In the
      recent years, Apache Spark has emerged as a solid foundation for data
      science and has taken the big data analytics domain by storm. We believe
      that the primitives exposed by Apache Spark can help software engineering
      researchers create and share reproducible, high-performance data analysis
      pipelines.

      In our technical briefing, we discuss how researchers can profit from Apache
      Spark, through a hands-on case study.
  },
  numpages = {2},
  doi = {10.1145/3183440.3183458},
  keywords = {data analytics; big data; Apache Spark},
  url = {/pub/sw-analytics-spark.pdf}
}

@inproceedings{SG18,
  author = {Spinellis, Diomidis and Gousios, Georgios},
  title = {How to Analyze Git Repositories with Command Line Tools: Weâ€™re not in Kansas Anymore},
  booktitle = {Proceedings of the 40th International Conference on Software Engineering Companion},
  series = {ICSE-C '18},
  year = {2018},
  location = {Gothenburg, Sweden},
  abstract = {

  Git repositories are an important source of empirical so ware engineering
    product and process data. Running the Git command-line tool and processing
    its output with other Unix tools allows the incremental construction of
    sophisticated data processing pipelines. Git data analytics on the
    command-line can be systematically presented through a pattern that involves
    fetching, selection, processing, summarization, and reporting. For each part
    of the processing pipeline, we examine the tools and techniques that can be
    most effectively used to perform the task at hand. The presented techniques
    can be easily applied,  first to get a feeling of version control repository
    data at hand and then also for extracting empirical results.

  },
  numpages = {2},
  doi = {10.1145/3183440.3183469},
  keywords = {Git; data analytics; command-line tools; pipes and filters;
    empirical software engineering},
  url = {/pub/git-repos-cmdline-tools.pdf}
}

@inproceedings{GS17,
  author = {Gousios, Georgios and Spinellis, Diomidis},
  title = {Mining Software Engineering Data from GitHub},
  booktitle = {Proceedings of the 39th International Conference on Software Engineering Companion},
  series = {ICSE-C '17},
  year = {2017},
  isbn = {978-1-5386-1589-8},
  location = {Buenos Aires, Argentina},
  pages = {501--502},
  abstract = {
  GitHub is the largest collaborative source code hosting site built on top of
    the Git version control system. The availability of a comprehensive API has
    made GitHub a target for many software engineering and online collaboration
    research efforts. In our work, we have discovered that a) obtaining data
    from GitHub is not trivial, b) the data may not be suitable for all types of
    research, and c) improper use can lead to biased results. In this tutorial,
  we analyze how data from GitHub can be used for large-scale, quantitative
    research, while avoiding common pitfalls. We use the GHTorrent dataset, a
    queryable offline mirror of the GitHub API data, to draw examples from and
    present pitfall avoidance strategies.
  },
  numpages = {2},
  doi = {10.1109/ICSE-C.2017.164},
  publisher = {IEEE Press},
  address = {Piscataway, NJ, USA},
  keywords = {GHTorrent, GitHub, empirical software engineering, git},
  url = {/pub/mining-soft-eng-data-github.pdf},
  github = {ghtorrent/tutorial}
}
