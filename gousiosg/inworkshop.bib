@inproceedings{HLSRGR17,
  author = {Hennie Huijgens and Robert Lamping and Dick Stevens and Hartger
    Rothengatter and Georgios Gousios and Daniele Romano},
  title = {Strong Agile Metrics: Mining Log Data to Determine Predictive Power of Software Metrics for Continuous Delivery Teams},
  booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
  series = {ESEC/FSE 2017},
  year = {2017},
  isbn = {978-1-4503-5105-8},
  location = {Paderborn, Germany},
  pages = {866--871},
  numpages = {6},
  doi = {10.1145/3106237.3117779},
  acmid = {3117779},
  publisher = {ACM},
  address = {New York, NY, USA},
  abstract = {

    ING Bank, a large Netherlands-based internationally operating bank,
    implemented a fully automated continuous delivery pipe-line for its software
    engineering activities in more than 300 teams, that perform more than 2500
    deployments to production each month on more than 750 different
    applications. Our objective is to examine how strong metrics for agile
    (Scrum) DevOps teams can be set in an iterative fashion. We perform an
    exploratory case study that focuses on the classification based on
    predictive power of software metrics, in which we analyze log data derived
    from two initial sources within this pipeline. We analyzed a subset of 16
    metrics from 59 squads. We identified two lagging metrics and assessed
    four leading metrics to be strong.

  },
  url = {/pub/strong-agile-metrics.pdf},
}

@inproceedings{BGZ17a,
  author = {Moritz Beller and Georgios Gousios and Andy Zaidman},
  title = {TravisTorrent: Synthesizing Travis CI and GitHub for Full-Stack Research on Continuous Integration},
  booktitle = {Proceedings of the 14th Working Conference on Mining Software Repositories},
  series = {MSR'17},
  year = {2017},
  publisher = {IEEE press},
  pages = {447--450},
  doi = {10.1109/MSR.2017.24},
  location = {Buenos Aires, Argentina},
  abstract = {

   Continuous Integration (CI) has become a best practice of modern software
   development. Thanks in part to its tight integration with GitHub, Travis CI
   has emerged as arguably the most widely used CI platform for Open-Source
   Software (OSS) development. However, despite its prominent role in Software
   Engineering in practice, the benefits, costs, and implications of doing CI
   are all but clear from an academic standpoint. Little research has been done,
   and even less was of quantitative nature. In order to lay the groundwork for
   data-driven research on CI, we built TravisTorrent,
   travistorrent.testroots.org, a freely available data set based on Travis CI and
   GitHub that provides easy access to hundreds of thousands of analyzed builds
   from more than 1,000 projects. Unique to TravisTorrent is that each of its
   2,640,825 Travis builds is synthesized with meta data from Travis CI’s API,
   the results of analyzing its textual build log, a link to the GitHub commit
   which triggered the build, and dynamically aggregated project data from the
   time of commit extracted through GHTorrent.

  },
  url = {/pub/travistorrent.pdf},
}

@inproceedings{GSV16,
  author = {Gousios, Georgios and Safaric, Dominik and Visser, Joost},
  title = {Streaming Software Analytics},
  booktitle = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
  series = {BIGDSE '16},
  year = {2016},
  isbn = {978-1-4503-4152-3},
  location = {Austin, Texas},
  pages = {8--11},
  numpages = {4},
  url = {/pub/streaming-soft-analytics.pdf},
  doi = {10.1145/2896825.2896832},
  acmid = {2896832},
  publisher = {ACM},
  address = {New York, NY, USA},
  keywords = {data mining, software analytics, streaming},
}

@inproceedigns {BLPGZ16,
  author = {Moritz Beller and Igor Levaja and Annibale Panichella and Georgios Gousios and Andy Zaidman},
  title = {How to Catch ’Em All: WatchDog, a Family of IDE Plug-Ins to Assess Testing},
  booktitle = {Proceedings of the 3rd International Workshop on Software Engineering Research and Industrial Practice},
  year = 2016,
  Abstract = {
    As software engineering researchers, we are also zealous tool smiths.
    Building a research prototype is often a daunting task, let alone building a
    industry-grade family of tools supporting multiple platforms to ensure the
    generalizability of our results. In this paper, we give advice to academic
    and industrial tool smiths on how to design and build an easy-to-maintain
    architecture capable of supporting multiple integrated development
    environments (IDEs). Our experiences stem from WatchDog, a multi-IDE
    infrastructure that assesses developer testing activities in vivo and that
    over 2,000 registered developers use. To these software engineering
    practitioners, WatchDog, provides real-time and aggregated feedback in the
    form of individual testing reports.
  },
  url = {/pub/how-to-catchem-all.pdf}
}

@inproceedings{BGZ15,
  author = {Moritz Beller and Georgios Gousios and Andy Zaidman},
  title = {How (Much) Do Developers Test?},
  booktitle = {Proceedings of the 37th International Conference on Software Engineering -- Niew Ideas and Emerging Results track},
  year = {2015},
  month={May},
  volume={2},
  pages={559-562},
  doi={10.1109/ICSE.2015.193},
  Abstract = {
    What do we know about software testing in the real world? It seems we know
    from Fred Brooks’ seminal work ``The Mythical Man-Month'' that 50\% of
    project effort is spent on testing. However, due to the enormous advances in
    software engineering in the past 40 years, the question stands: Is this
    observation still true? In fact, was it ever true? The vision for our
    research is to settle the discussion about Brooks' estimation once and for
    all: How much do developers test? Does developers' estimation on how much
    they test match reality? How frequently do they execute their tests, and is
    there a relationship between test runtime and execution frequency? What are
    the typical reactions to failing tests? Do developers solve actual defects
    in the production code, or do they merely relax their test assertions?
    Emerging results from 40 software engineering students show that students
    overestimate their testing time threefold, and 50\% of them test as little
    as 4\% of their time, or less. Having proven the scalability of our
    infrastructure, we are now extending our case study with professional
    software engineers from open-source and industrial organizations.
  },
  url = {/pub/test-time-nier.pdf}
}

@inproceedings{HG15,
  author = {Claudia Hauff and Georgios Gousios},
  title = {Matching GitHub developer profiles to job advertisements},
  booktitle = {Proceedings of the 2015 IEEE/ACM 12th Working Conference on Mining Software Repositories (MSR)},
  year = {2015},
  month={May},
  location = {Florence, Italy},
  Abstract = {
    GitHub is a social coding platform that enables developers to efficiently
    work on projects, connect with other developers, collaborate and generally
    ``be seen'' by the community. This visibility also extends to prospective
    employers and HR personnel who may use GitHub to learn more about a
    developer’s skills and interests. We propose a pipeline that automatizes
    this process and automatically suggests matching job advertisements to
    developers, based on signals extracting from their activities on GitHub.
  },
  doi={10.1109/MSR.2015.41},
  pages={362-366},
  url = {/pub/dev-profiles.pdf}
}

@inproceedings{VGZ15,
  author = {Erik van der Veen and Georgios Gousios and Andy Zaidman},
  title = {Automatically Prioritizing Pull Requests},
  booktitle = {Proceedings of the 2015 IEEE/ACM 12th Working Conference on Mining Software Repositories (MSR)},
  year = {2015},
  month={May},
  location = {Florence, Italy},
  abstract = {
    In previous work, we observed that in the pull-based development model
    integrators face challenges with regard to prioritizing work in the face of
    multiple concurrent pull requests. We present the design and initial
    implementation of a prototype pull request prioritisation tool called
    PRioritizer. PRioritizer works like a priority inbox for pull requests,
    recommending the top pull requests the project owner should focus on. A
    preliminary user study showed that PRioritizer provides functionality that
    GitHub is currently lacking, even though users need more insight into how
    the priority ranking is established to make PRioritizer really useful.
  },
  doi={10.1109/MSR.2015.40},
  pages={357-361},
  url = {/pub/prioritizer.pdf},
  github = {PRioritizer/PRioritizer-paper}
}

@inproceedings{CDGHH15,
  author = {Guanliang Chen and Dan Davis and Georgios Gousios and Claudia Hauff and Geert-Jan Houben},
  title = {Learning transfer: does it take place?},
  booktitle = {Proceedings of the 2nd Workshop of Learning With MOOCs},
  year = {2015},
  month={Oct},
  location = {NY, USA},
  Abstract = {
    The rising number of MOOCs enable people to advance their knowledge and
    competencies in a wide range of fields. Learning though is only the first
    step, the transfer of the taught concepts into practice is equally important
    and often ne- glected in the investigation of MOOCs. In this paper, we
    consider the specific case of FP101x (a functional programming MOOC on
    EdX) and the extent to which learners alter their programming behaviour
    after having taken the course. We find the practical uptake six months after
    the course to be small: only ~3\% of learners that were not applying
    functional concepts in their programming practice before the start of
    FP101x, begin using them.
  },
  url = {/pub/learning-transfer.pdf}
}

@inproceedings{KMLGS15,
  author = {Karakoidas, Vassilios and Mitropoulos, Dimitrios and Louridas, Panos and Gousios, Georgios and Spinellis, Diomidis},
  title = {Generating the Blueprints of the Java Ecosystem},
  booktitle = {Proceedings of the 2015 IEEE/ACM 12th Working Conference on Mining Software Repositories (MSR)},
  year = {2015},
  month = {May},
  location = {Florence, Italy},
  pages = {510-513},
  abstract = {
    Examining a large number of software artifacts can provide the research
    community with data regarding quality and design. We present a dataset
    obtained by statically analyzing 22730 jar files taken from the Maven
    central archive, which is the de-facto application library repository for
    the Java ecosystem. For our analysis we used three popular static analysis
    tools that calculate metrics regarding object-oriented design, program size,
    and package design. The dataset contains the metrics results that every tool
    reports for every selected jar of the ecosystem. Our dataset can be used to
    produce interesting research results, such as measure the domain-specific
    language usage.},
  doi={10.1109/MSR.2015.76},
  url={/pub/generating-blueprints-java-ecosystem.pdf}
}

@inproceedings{MKPLGS14,
  author = {Dimitris Mitropoulos and Georgios Gousios and Panagiotis Papadopoulos and Vassilios Karakoidas and Panos Louridas and Diomidis Spinellis},
  title = {The Vulnerability Dataset of a Large Software Ecosystem},
  booktitle = {Proceedings of the 3rd International Workshop on Building Analysis Datasets and Gathering Experience Returns for Security},
  year = {2014},
  month = {September},
  location = {Wroclaw, Poland},
  publisher = {IEEE Computer Society},
  url = {/pub/maven-bug-dataset.pdf}
}

@inproceedings{GVSZ14,
  author = {Gousios, Georgios and Vasilescu, Bogdan and Serebrenik, Alexander and Zaidman, Andy},
  title = {Lean {GHT}orrent: GitHub Data on Demand},
  booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
  series = {MSR 2014},
  year = {2014},
  isbn = {978-1-4503-2863-0},
  location = {Hyderabad, India},
  pages = {384--387},
  numpages = {4},
  doi = {10.1145/2597073.2597126},
  acmid = {2597126},
  publisher = {ACM},
  address = {New York, NY, USA},
  keywords = {GitHub, data on demand, dataset},
  Abstract = {
    In recent years, Github has become the largest code host in the world, with
    more than 5M developers collaborating across 10M repositories.  Numerous
    popular open source projects (such as Ruby on Rails, Homebrew, Bootstrap,
    Django or JQuery) have chosen Github as their host and have migrated their
    code base to it. Github offers a tremendous research potential.  For
    instance, it is a flagship for current open source development, a place for
    developers to showcase their expertise to peers or potential recruiters, and
    the platform where social coding features or pull requests emerged.
    However, Github data is, to date, largely underexplored. To facilitate
    studies of Github, we have created GHTorrent, a scalable, queriable, offline
    mirror of the data offered through the Github REST API. In this paper we
    present a novel feature of GHTorrent designed to offer customisable data
    dumps on demand. The new GHTorrent data-on-demand service offers users the
    possibility to request via a web form up-to-date GHTorrent data dumps for
    any collection of Github repositories. We hope that by offering customisable
    GHTorrent data dumps we will not only lower the "barrier for entry" even
    further for researchers interested in mining Github data (thus encourage
    researchers to intensify their mining efforts), but also enhance the
    replicability of Github studies (since a snapshot of the data on which the
    results were obtained can now easily accompany each study).
  },
  url = {/pub/lean-ghtorrent.pdf},
  github = {ghtorrent/ghtorrent-service},
  speakerdeck = "992bb730cd090131fa3126624a8aace7"
}

@inproceedings{GZ14,
  author = {Gousios, Georgios and Zaidman, Andy},
  title = {A Dataset for Pull-based Development Research},
  booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
  series = {MSR 2014},
  year = {2014},
  isbn = {978-1-4503-2863-0},
  location = {Hyderabad, India},
  pages = {368--371},
  numpages = {4},
  doi = {10.1145/2597073.2597122},
  acmid = {2597122},
  publisher = {ACM},
  address = {New York, NY, USA},
  keywords = {distributed software development, empirical software engineering, pull request, pull-based development},
  Abstract = {
    Pull requests form a new method for collaborating in distributed software
    development. To study the pull request distributed development model, we
    constructed a dataset of almost 900 projects and 350,000 pull requests,
    including some of the largest users of pull requests on Github. In this
    paper, we describe how the project selection was done, we analyze the
    selected features and present a machine learning tool set for the R
    statistics environment.
  },
  url = {/pub/pullreqs-dataset.pdf},
  award = {MSR2014: Best data showcase paper},
  github = {gousiosg/pullreqs},
  speakerdeck = "629aa910cd09013116791efd7f77c4b7"
}

@inproceedings{MKLGS14,
  author = {Mitropoulos, Dimitris and Karakoidas, Vassilios and Louridas, Panos and Gousios, Georgios and Spinellis, Diomidis},
  title = {The Bug Catalog of the Maven Ecosystem},
  booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
  series = {MSR 2014},
  year = {2014},
  isbn = {978-1-4503-2863-0},
  location = {Hyderabad, India},
  pages = {372--375},
  numpages = {4},
  doi = {10.1145/2597073.2597123},
  acmid = {2597123},
  publisher = {ACM},
  address = {New York, NY, USA},
  keywords = {FindBugs, Maven Repository, Software Bugs},
  Abstract = {
    Examining software ecosystems can provide the research community with data
    regarding artifacts, processes, and communities. We present a dataset
    obtained from the Maven central repository ecosystem (approximately 265{GB}
    of data) by statically analyzing the repository to detect potential software
    bugs. For our analysis we used {F}indBugs, a tool that examines Java
    bytecode to detect numerous types of bugs. The dataset contains the metrics
    results that Find- Bugs reports for every project version (a jar) included
    in the ecosystem. For every version we also stored specific metadata such as
    the jar’s size, its dependencies and others. Our dataset can be used to
    produce interesting research results, as we show in specific examples.},
  url = {/pub/maven-findbugs.pdf},
}

@inproceedings{MKLGS13,
  Author = {Dimitris Mitropoulos and Vassilios Karakoidas and Panos Louridas and Georgios Gousios and Diomidis Spinellis},
  Title = {Dismal Code: Studying the Evolution of Security Bugs},
  Year = {2013},
  Month = {Oct},
  Booktitle = { {LASER} '13: Proceedings of the 2013 Workshop on Learning from Authoritative Security Experiment Results},
  address = {Arlington, VA},
  isbn = {978-1-931971-06-5},
  pages = {37--48},
  abstract = {
    Security bugs are critical programming errors that can lead to serious
    vulnerabilities in software. Such bugs may allow an attacker to take over an
    application, steal data or prevent the application from working at all.  We
    used the projects stored in the Maven repository to study the
    characteristics of security bugs individually and in relation to other
    software bugs. Specifically, we studied the evolution of security bugs
    through time. In addition, we examined their persistence and their
    relationship with a) the size of the corresponding version, and b) other bug
    categories.  We analyzed every project version of the Maven repository by
    using FindBugs, a popular static analysis tool.  To see how security bugs
    evolve over time we took advantage of the repository's project history and
    dependency data.  Our results indicate that there is no simple rule
    governing the number of security bugs as a project evolves.  In particular,
    we cannot say that across projects security-related defect counts increase
    or decrease significantly over time.  Furthermore, security bugs are not
    eliminated in a way that is particularly different from the other bugs. In
    addition, the relation of security bugs with a project's size appears to be
    different from the relation of the bugs coming from other categories.
    Finally, even if bugs seem to have similar behaviour, severe security bugs
    seem to be unassociated with other bug categories.  Our findings indicate
    that further research should be done to analyze the evolution of security
    bugs. Given the fact that our experiment included only Java projects,
    similar research could be done for another ecosystem. Finally, the fact that
    projects have their own idiosyncrasies concerning security bugs, could help
    us find the common characteristics of the projects where security bugs
    increase over time.
  },
  url = {/pub/evolution-security-bugs.pdf}
}

@inproceedings{MGS12,
  Title={Measuring the Occurrence of Security-Related Bugs through Software
    Evolution},
  Author={Dimitris Mitropoulos and Georgios Gousios and Diomidis
    Spinellis},
  Booktitle={PCI 2012: Proceedings of 16th Panhellenic Conference on
    Informatics},
  Publisher={IEEE Computer Society},
  Year = {2012},
  doi={10.1109/PCi.2012.15},
  Pages = {117--122},
  abstract = "A security-related bug is a programming error that intro- duces a
    potentially exploitable weakness into a computer system. This weakness could
    lead to a security breach with unfortunate consequences. Version control
    systems provide an accurate historical record of the software code’s evolu-
    tion. In this paper we examine the frequency of the security-related bugs
    throughout the evolution of a software project by applying the FindBugs static
    analyzer on all versions of its revision history. We have applied our approach
    on four projects and we have come out with some interesting results including
    the fact that the number of the security-related bugs increase as the project
    evolves.",
  url = {/pub/measuring-the-occurrence-of-security-related-bugs-through-software-evolution.pdf}
}

@inproceedings{KGSP09,
  Author = {Eirini Kalliamvakou and Georgios Gousios and Diomidis Spinellis and Nancy Pouloudi},
  Booktitle = {MCIS 2009: 4th {M}editerranean Conference on Information Systems},
  Editor = {Poulymenakou, A. and Pouloudi, N. and Pramatari, K.},
  Location = {Athens, Greece},
  Month = sep,
  Pages = {600--611},
  Title = {Measuring Developer Contribution from Software Repository Data},
  Year = 2009,
  abstract = {
    Our work is concerned with an enriched perspective of what constitutes
    developer contribution in software infrastructures supporting incremental
    development and distributed software projects. We use the term
    “contribution” to express the combination of all the actions a developer has
    performed during the development process and propose a model for calculating
    this individually for developers participating in a software project. Our
    approach departs from the traditional practice of only measuring the
    contribution to the final outcome (the code) and puts emphasis additionally
    on other activities that do not directly affect the product itself but are
    essential to the development process. We use the Open Source Software (OSS)
    context to take advantage of the public availability of data in software
    repositories. In this paper, we present our method of calculation and its
    system implementation and we apply our measurements on various projects from
    the GNOME ecosystem.
    },
  Url = {/pub/measuring-developer-contribution-from-software-repository-data.pdf}
}

@inproceedings{GS09a,
  Author = {Georgios Gousios and Diomidis Spinellis},
  Booktitle = { {ICSE} '09: Proceedings of the 31st International Conference on Software Engineering -- Formal Research Demonstrations Track},
  Day = {16--24},
  Isbn = {978-1-4244-3743-6},
  Location = {Vancouver, Canada},
  Month = {May},
  Pages = {579--582},
  Publisher = {IEEE},
  doi={10.1109/ICSE.2009.5070560},
  Title = {Alitheia Core: An extensible software quality monitoring platform},
  Year = {2009},
  abstract = {
    Research in the fields of software quality and maintainability requires the
    analysis of large quantities of data, which often originate from open source
    software projects. Pre-processing data, calculating metrics, and
    synthesizing composite results from a large corpus of project artefacts is a
    tedious and error prone task lacking direct scientific value. The Alitheia
    Core tool is an extensible platform for software quality analysis that is
    designed specifically to fa- cilitate software engineering research on large
    and diverse data sources, by integrating data collection and preprocess- ing
    phases with an array of analysis services, and presenting the researcher
    with an easy to use extension mechanism. The system has been used to process
    several projects successfully, forming the basis of an emerging ecosystem of
    quality analysis tools.},
  url =
  {/pub/alitheia-core-extensible-software-quality-monitoring-platform.pdf},
  github = {istlab/Alitheia-Core}
}

@inproceedings{GS08,
  Author = {Georgios Gousios and Diomidis Spinellis},
  Booktitle = {Proceedings of the 12th Pan-Hellenic Conference on Informatics},
  Title = {Java Performance Evaluation Using External Instrumentation},
  Year = {2008},
  Location = {Samos, Greece},
  doi={10.1109/PCI.2008.14},
  pages={173-177},
  Month = {Aug},
  abstract = {
    The performance of programs written in the Java pro- gramming language is
    not trivial to analyse. The Java Vir- tual Machine hides the details of
    bytecode execution while not providing an accessible profiling mechanism.
    Most tools used for Java performance evaluations are based on sampling and
    only present engineers with sampled data aggregations. In this paper, we
    present the Java DTrace Toolkit, a collection of scripts that is
    specifically designed to assist engineers in identifying the roots of
    various perfor- mance problems observed with other tools.
  },
  url = {/pub/java-performance-evaluation-using-external-instrumentation.pdf}
}

@inproceedings{GKS08,
  Address = {New York, NY, USA},
  Author = {Georgios Gousios and Eirini Kalliamvakou and Diomidis Spinellis},
  Booktitle = {MSR '08: Proceedings of the 2008 International Working Conference on Mining Software Repositories},
  Isbn = {978-1-60558-024-1},
  Location = {Leipzig, Germany},
  Pages = {129--132},
  Publisher = {ACM},
  doi = {10.1145/1370750.1370781},
  Title = {Measuring developer contribution from software repository data},
  Year = {2008},
  abstract = {
    Apart from source code, software infrastructures supporting agile and
    distributed software projects contain traces of developer activity that does
    not directly affect the product itself but is important for the development
    process. We propose a model that, by combining traditional contribution
    metrics with data mined from software repositories, can deliver accurate
    developer contribution measurements. The model creates clusters of similar
    projects to extract weights that are then applied to the actions a developer
    performed on project assets to extract a combined measurement of the
    developer’s contribution. We are currently implementing the model in the
    context of a software quality monitoring system while we are also validating
    its components by means of questionnaires.
  },
  url = {/pub/measuring-developer-contribution-from-repository-data.pdf}
}

@inproceedings{GKSLVS07,
  Author = {Georgios Gousios and Vassilios Karakoidas and Konstantinos Stroggylos and Panagiotis Louridas and Vasileios Vlachos and Diomidis Spinellis},
  Booktitle = {Proceedings of the 11th Panhellenic Conference on Informatics},
  Day = {18--20},
  Location = {Patras, Greece},
  Month = may,
  Title = {Software Quality Assessment of Open Source Software},
  Year = {2007},
  abstract = {
    The open source software ecosystem comprises more than a hundred thousand
    applications of varying quality. Individuals and organizations wishing to
    use open source software packages have scarce objective data to evaluate
    their quality. However, open source development projects by definition allow
    anybody to read, and therefore evaluate their source code. In addition, most
    projects also publish process-related artefacts, such as bug databases,
    mailing lists, and configuration management system logs. The software
    quality observatory is a platform that uses these product and process data
    sources to automatically evaluate the quality of open source projects. A
    plugin-based service-oriented architecture allows the mixing and matching of
    metrics extraction suites, source code repositories, and transformation
    filters. The resulting platform is aimed at IT consultants and managers, the
    open source community, and researchers.
  },
  url = {/pub/software-quality-assessment-of-open-source-software.pdf}
}

